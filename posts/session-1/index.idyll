[meta title: "CS112 - Session 1" /]

[Header
  fullWidth: true
  title:"Session 1"
  subtitle:"Introduction to the course"
  background:"#222222"
  color:"#ffffff"
   /]

[HomeButton /]
[LoadExercises /]
[LearningOutcomes los:`["decisiondata", "decisionbrief"]` /]

# Readings
[Resource
  name: "Howard, J. (2012). From predictive modelling to optimization The next frontier"
  description: "Howard explains how a successful entrepreneur and data scientists used big data and insights about prediction and causal inference to transform the way insurance was sold. In so doing, they built an enormously successful company. He distills his lessons into a concrete framework called the “drivetrain” model. Students should be sure to understand this model and know how to use it (i.e., understand the purpose of each component)."
  link: "www.youtube.com/watch?v=vYrWTDxoeGg&feature=youtu.be" /]

building a predicative model - how to deliver value?

### The Drivetrain Model

only ruly random data can demonstrate causal relationships
1. identify the objective
2. figure out what levers you have to pull - things that can bring change
3. what data do we have and can collect that helps pull the levers (causal data)
4. hook together the the data, lever, and objective, using a modeller, a simulator, an optimizer

// [Resource
//   name: "Varian, H. R. (2014). Beyond big data. Business Economics, 49(1)."
//   description: "This 5-page paper explains in simple, but technical terms, how computer-mediated data is being used by data scientists. The author is Google’s Chief Economist."
//   link: "link.springer.com/content/pdf/10.1057/be.2014.1.pdf" /]

// ### Four uses of big data:

// **Data extraction and analysis.**
//     - tools for data manipulation
//     - methods for data analysis, from ML
//     - barrier to entry is lowered (cloud hardware, database tools, analysis tools, and developer support), new entrants in the data analysis area
//     - cool Netflix challenge & Kaggle:

//         Back in 2006, Netflix realized that 75 percent of movie views in its library were driven by recommendations. They created the “Netflix Prize” of $1 million that would be awarded to the group that developed the best machine learning system for recommendations, as long as it improved the current version by at least 10 percent. They provided training data of about 100M ratings, 500,000 users, and 1,800 movies. A year later, the prize was won by a team that blended 800 statistical models together using model averaging. The success of the NetFlix challenge led to the establishment of a startup named Kaggle, who will set up NetFlix-like challenges. (Note: I am an investor and an adviser to Kaggle.)

//         Kaggle puts the two sides of the market together. They now have 114,000 data scientists who tackle the submitted problems. Their motto is “We make data science a sport.”

//     - combining Data+Tools+Techniques +Expertise
// **Personalization and customization**
//     - These personalized searches, services, and ads have the potential of revolutionizing marketing.
//     - data privacy issues? example: personal assistants
// **Continuous Experiments**
//     - correlation vs causality, observational vs interventional experiments and data; you can only find causal relationships by doing experiments
//         - even better - randomly assigned treatment-control experiments; carried on continuously
//     - the web makes this easy: assign treatment and control groups based on traffic, cookies, usernames, geographic areas, and so on
//     - if you run experiments continuously, you can continuously improve your system
//     - Google's experiments:

//         - Google runs about 10,000 experiments a year in
//         search and ads. There are about 1,000 running at any
//         one time, and when you access Google you are in
//         dozens of experiments.

//         - What types of experiments? There are many:
//           - user interface experiments
//           - ranking results search and ads
//           - feature experiments
//           - product design

//         - tuning experiments

//         - All of these can be ongoing experiments or special
//         purpose experiments. Google has been so successful
//         with our own experiments that we have made them
//         available to our advertisers and publishers in two
//         programs. The first, Advertiser Campaign Experiments
//         (ACE), allows advertisers to experiment with bids,
//         budgets, creatives,2 and so on, in order to find the
//         optimal settings for their ads. The second, Contents
//         Experiment Platform, is part of Google Analytics; it
//         allows publishers to experiment with different web
//         page designs to find the one(s) that perform best for
//         them.

//     - types of experiments:
//         - user interface experiments
//         - ranking results search and ads
//         - feature experiments
//         - product design
//         - tuning experiments
// **New contractual forms due to better monitoring.**
//     - Computer (or cash register)-mediated transactions enable “trust...but verify” and allow more transactions to take place.
//     - trust and verification; examples: Uber and AirBnB

[Resource
  name: "STed Talk. (2010), February. Esther Duflo Social experiments to fight poverty [Video file]"
  description: "This 16-minute video explains (and advocates for) a scientific approach--centered around hypothesis-testing and experimental design/implementation--to international economic development program design."
  link: "www.ted.com/talks/esther_duflo_social_experiments_to_fight_poverty?language=en" /]

[Resource
  name: "Abbanat, C.. Guidelines for writing decision memos (n.d.)"
  description: "In every CS112 lesson (including this one), students will be thinking through the design of decisionmaking processes and tools. This concise and highly readable decision memo guide will help sharpen students’ ability to articulate their proposals and arguments. It will also be very helpful for working through the first assignment which will be posted by the first day of class."
  link: "ocw.mit.edu/courses/urban-studies-and-planning/11-027-city-to-city-comparing-researching-and-writing-about-cities-new-orleans-spring-2011/related-resources/MIT11_027S11_decision_memo.pdf" /]

How to write a decision-making memo?

1. Analyze your audience and define your purpose: purpose for writing + audience who is receiving the message + main message
2. Generate ideas: rainstorming; free writing technique; questioning as many questions as possible; dictating into a recorder
3. Group information under headlines: categories → headings
4. Sequence your ideas: points need to be in an effective order; one good order is decreasing levels of importance
5. Write a first draft
6. Edit for clarity, conciseness, accuracy, visual design, and tone.

[Resource
  name: "Rubin, D. B. (2003). Basic concepts of statistical inference for causal effects in experiments and observational studies.Cambridge, MA Harvard University, Department of Statistics. Read pages 1-10 (up through I-3.2)."
  description: "This reading was designed for Harvard University undergrads interested in the use of statistical methods for estimating cause-and-effects relationships. We have been granted permission to use it by the author, Donald Rubin, who is an enormously important figure in this field and whose name will arise repeatedly in this course.Understand the Perfect Doctor setting and familiarize yourself with the potential outcomes framework. Why is that the average difference in observed results (Y1 - Y0) does not match the true average treatment effect?"
  link: "www.stat.columbia.edu/~cook/qr33.pdf" /]


the 'Rubin Causal Model', key differentiators:
    1. Potential outcomes define causal effects in all cases: randomized experiments and observational studies
    2. Model for the assignment mechanism must be explicated for all forms of interference
    3. The framework allows specification of a joint distribution of the potential outcomes

### The Potential Outcomes Framework

- Unit: The person, place, or thing upon which a treatment will operate, at a particular time (one thing at two times = two units)
- Treatment: The intervention upon the unit, the effects of which we want to assess relative to no intervention (the control)
- Potential Outcomes: The values of a unit's measurement of interest after either application or non-application
- Causal Effect: For each unit, the comparison between the potential outcomes under treatment and under control

- does not involve probability
- is not a change over time

The Fundamental Problem of Causal Inference: We can observe at most one of the potential outcomes for each unit

Ways of measuring potential outcomes and causal effect:

- simple difference
    - potential outcomes = `Y(treatment)` and `Y(control)`
    - causal effect = `Y(treatment)-Y(control)`
- gain scores
    - potential outcomes = `Y(treatment) - X` (`X` = initial) and `Y(control) - X`
    - causal effect = `[Y(treatment)-X]-[Y(control)-X]`
- percent change
    - potential outcomes = `[X-Y(treatment)]/X 100%` and `[X-Y(control)]/X  100%`
    - causal effect = `[X-Y(treatment)]/X - [X-Y(control)]/X`

### Causal Effects and Assignment Mechanism

Replication
- at least one unit receives treatment and at least one unit receives control
- must-have in order to learn about causal effects

Stable Unit-Treatment-Value Assumption ("SUTVA")
1. there's only one form of both treatment and control
2. there is no interference among units

Measuring Population Level Causal Effects
- average causal effect
- median causal effect
- difference of median potential outcomes

Assignment Mechanism
- the process for deciding which unit receives what. critical!

Ignorable assignment mechanism
- the assignment is independent of the unobserved potential outcomes (vs nonignorable)
- essentially: the assignment does not care about what the potential outcomes may be, which is not known yet

Unconfounded assignment mechanism
- the assignment is independent of potential outcomes, observed or unobserved (vs confounded)
- essentially, the assignment does not care about what the potential outcomes are, which we might know


### The Perfect Doctor Setting

assignment mechanism is critical;
the importance of finding a statistic that is appropriate for the actual assignment mechanism

The perfect doctor chooses the best treatment for each patient, i.e., the treatment under which the patient will live longer. If there is no difference, he chooses by flipping a coin. The treatment each unit receives depends on that unit's potential outcomes.

- potential outcomes are observed
- assignment mechanism is nonignorable and thus confounded

The results are NOT representative and misleading; using difference of sample means is not right

Why is that the average difference in observed results `(Y1 - Y0)` does not match the true average treatment effect?
- because treatment was given based on potential outcomes, it is nonignorable and confounded


[Resource
  name: "James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An introduction to statistical learning With applications in R."
  description: "We're going to be reading this text for the first several weeks of the course. The Introduction gives you an idea of what we're getting ourselves into!"
  link: "www-bcf.usc.edu/~gareth/ISL/ISLR%20First%20Printing.pdf" /]

Statistical learning refers to a vast set of tools for understanding data. These tools can be classified as supervised or unsupervised. Broadly speaking, supervised statistical learning involves building a statistical model for predicting, or estimating, an output based on one or more inputs. Problems of this nature occur in fields as diverse as business, medicine, astrophysics, and public policy. With unsupervised statistical learning, there are inputs but no supervising output; nevertheless we can learn relationships and structure from such data.

- principle components of data
- dimensions

# Study Guide

1. A Bosnian NGO is providing safety gear and training to homeless street kids who collect and sell recyclable trash by the kilo to neighborhood recycling centers. The NGO wants to establish whether their program is boosting the kids’ productivity. What kind of data would be relevant for this decision question, and where could it come from? How could one obtain computer-mediated data? How would the data fit into Jeremy Howard’s drivetrain framework?
2. In Bangladesh, affordable access to effective healthcare is a huge problem. In particular, since the 1990s, three districts (the Chittagong Hill Tracts, the only three hilly districts of the country) have lagged behind the rest of the country on various indicators from healthcare to education. There may be several reasons for this: discrimination against an ethnic minority, relatively poor transport infrastructure, etc. How might the drivetrain model help Bangladesh address this public policy problem? Ensure that you understand each of the model’s components (e.g., objective, levers, data, models, etc.).
3. Decide if you have any questions about the "Decision Memo" reading or Rubin's reading -- if you have questions, be prepared to ask them in class (and/or post them to Piazza).

# Pre-class Work

- familiarize with datasets in R
- finish DataCamp course
- read LSR articulate

Familiarize yourself with the "datasets" library in R. All the datasets in this library are automatically available in the R working environment, by default.

You may want to Google it and have the documentation for that package available in class.

Ideally, you would know how to quickly obtain information about a particular dataset in that package. For example, if I asked you to look up the beaver2 data set and run some numbers, you should know how to do that rather quickly...

Since this is the first class, be sure to have covered the prerequisites described in the syllabus:

(1) A reading in [ISLR](faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf) (the introduction, i.e., pages 1-14); (2) a reading in Rubin's "QR33" document (the same as the Rubin reading you were asked to read for this session) ; and (3) R skills.

# Class

1. making predictions
2. causal inference - causality: finding the effect of things on other things


[FullWidth]
  [Exercise
    height: 200
    marginSides: "50px"
    initial: "# print Hello World"
    hint: "use a print statement"/]
[/FullWidth]